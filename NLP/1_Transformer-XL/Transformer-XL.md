## 1. Abstract
 
>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling.   

트렌스포머는 long-term dependency 문제를 많이 극복했지만, 모델의 구조가 고정된 길이의 문맥에 제한됐다.

>
